{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jaLIq7zlYLui"
      },
      "outputs": [],
      "source": [
        "!unzip -q nuclei_segmentation_student_version.zip\n",
        "!unzip -q drive/MyDrive/data.zip\n",
        "!mv nuclei_segmentation_student_version/* ./\n",
        "!rm -r nuclei_segmentation_student_version/ sample_data/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from datetime import timedelta\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import resnet18\n",
        "from torchvision.transforms import transforms\n",
        "\n",
        "from datasets import CONSEP\n",
        "from resnet_unet import TwoEncodersOneDecoder\n",
        "from utils.plotting import plot_loss\n",
        "\n",
        "cuda_device = torch.device('cpu', 0)\n",
        "\n",
        "\n",
        "def save_checkpoint(model, name):\n",
        "    checkpoint = {'model': model.state_dict()}\n",
        "    torch.save(checkpoint, f'drive/MyDrive/{name}.pth')\n",
        "\n",
        "\n",
        "def dice_loss_fn(x: torch.Tensor, target: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"Soft dice loss = 2*|Aâˆ©B| / |A|+|B|\"\"\"\n",
        "    eps = 1e-7\n",
        "    numerator = 2 * (x * target).sum((1, 2))\n",
        "    denominator = (x + target).sum((1, 2))\n",
        "\n",
        "    dice = 1 - (numerator + eps) / (denominator + eps)\n",
        "    return dice\n",
        "\n",
        "\n",
        "def train():\n",
        "    model = TwoEncodersOneDecoder(resnet18, pretrained=True, out_channels=1)\n",
        "    model.train()\n",
        "    model.to(cuda_device)\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToPILImage(),\n",
        "        transforms.RandomPosterize(3),\n",
        "        transforms.RandomEqualize(),\n",
        "        transforms.GaussianBlur(3),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.ColorJitter(brightness=[0.5, 1.5], contrast=[0.5, 1.5], saturation=[0.5, 1.5], hue=[-0.3, 0.3])\n",
        "    ])\n",
        "\n",
        "    dataset = CONSEP('/content/train', 'train', transform=transform)\n",
        "    dataloader = DataLoader(dataset, batch_size=64, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "    dataset_val = CONSEP('/content/val', 'val')\n",
        "    dataloader_val = DataLoader(dataset_val, batch_size=32, num_workers=2, pin_memory=True)\n",
        "\n",
        "    num_epochs = 20\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-3, weight_decay=1e-4)\n",
        "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, (len(dataloader) * num_epochs))\n",
        "\n",
        "    best_dice_score = 0\n",
        "    best_epoch = 0\n",
        "    bce_losses = []\n",
        "    dice_losses = []\n",
        "    for epoch in range(0, num_epochs):\n",
        "        print(f'Epoch: {epoch}')\n",
        "        epoch_start_time = time.time()\n",
        "        for batch_idx, (x, h_x, y) in enumerate(dataloader, 1):\n",
        "            # TODO: Step 1) Move x, h_x, and y to GPU\n",
        "            # TODO: Step 2) Convert h_x to have 3 channels by repeating the 1 channel it has 3 times.\n",
        "            #               Hint: You can use h_x.expand() function to do that without increasing memory usage\n",
        "            #                     or use the .repeat() function\n",
        "            # TODO: Step 3) Run the model and get the outputs.\n",
        "            # TODO: Step 4) a) Call the loss functions bce_loss_fn & dice_loss_fn. Add them to get the loss.\n",
        "            #                  The loss should be a single number (not an array).\n",
        "            #                   Hint: Use .mean() on dice_loss_fn's output\n",
        "            #               b) Append the loss values to their respective lists for plotting.\n",
        "            #                  Use .item() while appending the values.\n",
        "            # TODO: Step 5) Run the backward() pass on the loss function\n",
        "            # TODO: Step 6) Call the optimizer to update the model and then zero out the gradients.\n",
        "\n",
        "            # The lines below prints loss values every 5 batches.\n",
        "            # Uncomment them to see the loss go down during training.\n",
        "\n",
        "            # if batch_idx % 5 == 0 or batch_idx == len(dataloader) - 1:\n",
        "            #     print(f'{epoch}-{batch_idx:03}\\t{round(bce_loss.item(), 6)} {round(dice_loss.item(), 6)} ', flush=True)\n",
        "\n",
        "        scheduler.step()\n",
        "        print(f'Epoch {epoch} took {timedelta(seconds=time.time() - epoch_start_time)}', flush=True)\n",
        "\n",
        "        print('EVALUATING dice score on validation set')\n",
        "        eval_start_time = time.time()\n",
        "        dice_score_val = eval_dice_with_h_x(model, dataloader_val)\n",
        "        print(f'Evaluation after epoch {epoch} took {timedelta(seconds=time.time() - eval_start_time)}', flush=True)\n",
        "        if dice_score_val > best_dice_score:\n",
        "            best_epoch = epoch\n",
        "            best_dice_score = dice_score_val\n",
        "            print('Saving model as a new best score has been achieved.')\n",
        "            save_checkpoint(model, f'TwoEncodersOneDecoder_consep')\n",
        "\n",
        "    print(f'Best dice score achieved on validation dataset was {best_dice_score} for epoch {best_epoch}', flush=True)\n",
        "    # Save loss values in case the plotting throws an error or you wanna plot with different parameters\n",
        "    with open('bce_loss.npy', 'wb') as f:\n",
        "        np.save(f, np.array(bce_losses))\n",
        "    with open('dice_loss.npy', 'wb') as f:\n",
        "        np.save(f, np.array(dice_losses))\n",
        "    # Save loss plots\n",
        "    print('Saving plots')\n",
        "    plot_loss(bce_losses, 'bce_loss')\n",
        "    plot_loss(dice_losses, 'dice_loss')\n",
        "    print('Plots saved')\n",
        "\n",
        "\n",
        "def eval_dice_with_h_x(model, dataloader):\n",
        "    model.eval()\n",
        "    dice = []\n",
        "    for batch_idx, (x, h_x, y) in enumerate(dataloader):\n",
        "        # TODO: Move (x, h_x, y) to cuda\n",
        "        with torch.no_grad():\n",
        "            # TODO: Step 1) Convert h_x to have 3 channels just like you did in the train() function\n",
        "            # TODO: Step 2) Run the model and store outputs in the variable out below\n",
        "            out = None\n",
        "            # TODO: Step 3) Convert the outputs to a binary mask as follows:\n",
        "            #               a) Pass the output through the sigmoid function to get an output between 0 and 1\n",
        "            #               b) Using 0.5 as the threshold, convert the values to 0 if they are < 0.5 and 1 if > 0.5\n",
        "        dice.append(None)  # TODO: Replace None with the output of the dice_loss_fn called for the binary mask\n",
        "    dice_score = 1 - torch.cat(dice, 0).mean()\n",
        "    print(f'dice score (the higher the better): {dice_score}')\n",
        "    model.train()\n",
        "    return dice_score\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train()"
      ],
      "metadata": {
        "id": "Tc4l2ZerbGgz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}